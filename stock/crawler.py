import requests
import time
import asyncio
from lxml import etree

from common.utils import HEADERS
# HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
HEADERS1 = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36", "Referer":"https://stock.finance.sina.com.cn/forex/globalbd/gcny10.html"}

URL = 'https://legulegu.com/stockdata/marketcap-gdp'
URL1 = 'https://hq.sinajs.cn/?rn=1670838672882&list=globalbd_gcny10'
URL2 = 'https://stock.xueqiu.com/v5/stock/realtime/quotec.json?symbol=HKHSI,SH000001,.IXIC'
URL3 = 'https://xueqiu.com'
URL4 = 'https://stock.xueqiu.com/v5/stock/quote.json?symbol='
ZSH = 'SH600028'
NH = 'SH601288'
DQTL = 'SH601006'
YLGF = 'SH600887'
HLSN = 'SH600585'
SHFZ = 'SZ000895'
ZSYH = 'SH600036'
GZMT = 'SH600519'
YGER = 'SH600177'
TSG = 'SH601000'
TXKG = '00700'
LRZY = 'SH600285'
CJDL = 'SH600900'
SHJC = 'SH600009'
GLYY = 'SH603087'
SXG = 'SH603896'
YZGF = 'SH603886'
ZZHL = 'SH000922'
FAN = 'SZ002327'
MDJT = 'SZ000333'
XYYH = 'SH601166'
GLDQ = 'SZ000651'
SXMY = 'SH601225'
ZGPA = 'SH601318'

async def get_data_current(cookies, code):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(f'{URL4}{code}&extend=detail', headers=HEADERS, cookies=cookies)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        json_data = r.json()
        if json_data is None:
            return ''
        # print(json_data)
        quote = json_data['data']['quote']
        current = quote['current']
        chg = quote['chg']
        percent = quote['percent']
        low52w = quote['low52w']
        name = quote['name']
        market_capital = quote['market_capital']
        dividend_yield = quote.get('dividend_yield', 'æ— ')
        pb = quote.get('pb', 'æ— ')
        if chg >= 0.0:
            chg_str = f'ğŸ“ˆ{chg}'
        else:
            chg_str = f'ğŸ“‰{abs(chg)}'
        percent_str = ''
        if abs(percent) > 1.0:
            percent_str = f'ï¼Œ{str(percent)}ä¸ªç‚¹'

        cheap_value = int(100-(abs(current-low52w)/current)*100)
        if market_capital is None:
            market_capital_str = ''
        else:
            market_capital = int(market_capital / 100000000)
            market_capital_str = f'ï¼Œå¸‚å€¼{str(market_capital)}äº¿'


        ret = f"{name}: {str(current)}ï¼Œ{chg_str}{percent_str}{market_capital_str}ï¼Œæœ€ä½{low52w}ï¼Œpb{pb}ï¼Œè‚¡æ¯ç‡{str(dividend_yield)}ï¼Œä¾¿å®œåº¦{str(cheap_value)}"
        print(ret)
        return ret

async def get_all(cookie):
    f = await asyncio.gather(
        get_data_current(cookie, ZGPA),
        get_data_current(cookie, SXMY),
        get_data_current(cookie, GLDQ),
        get_data_current(cookie, XYYH),
        get_data_current(cookie, MDJT),
        get_data_current(cookie, FAN),
        get_data_current(cookie, YLGF),
        get_data_current(cookie, HLSN),
        get_data_current(cookie, SHFZ),
        get_data_current(cookie, NH),
        get_data_current(cookie, ZSH),
        get_data_current(cookie, DQTL),
        get_data_current(cookie, ZSYH),
        get_data_current(cookie, GZMT),
        get_data_current(cookie, TXKG),
        get_data_current(cookie, LRZY),
        get_data_current(cookie, GLYY),
        get_data_current(cookie, CJDL),
        get_data_current(cookie, SHJC),
        get_data_current(cookie, YZGF),
        get_data_current(cookie, YGER),
        get_data_current(cookie, TSG),
        get_data_current(cookie, SXG),
        get_data_current(cookie, ZZHL),
    )
    return f
class Crawler:
    def __init__(self) -> None:
        pass

    def monitor(self):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(URL3, headers=HEADERS)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        # ret = 'ä¸­å›½çŸ³åŒ–ï¼š'
        # ret += self._get_data_current(r.cookies, ZSH)
        # ret += '\n'
        # ret += 'å†œä¸šé“¶è¡Œï¼š'
        # ret += self._get_data_current(r.cookies, NH)
        # ret += '\n'
        # ret += 'å¤§ç§¦é“è·¯ï¼š'
        # ret += self._get_data_current(r.cookies, DQTL)
        # ret += '\n---------------\n'
        f = asyncio.run(get_all(r.cookies))
        return '\n'.join(f)

    def _get_data_current(self,cookies, code):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(f'{URL4}{code}', headers=HEADERS, cookies=cookies)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        json_data = r.json()
        if json_data is None:
            return ''
        return str(json_data['data']['quote']['current'])


    def get_gnp(self):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(URL, headers=HEADERS)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'è·å–å·´è²ç‰¹æŒ‡æ•°ï¼š\n'
        dom = etree.HTML(r.content)
        data = dom.xpath('//*[@id="data-description"]/text()')
        if len(data) > 0:
            gnp = data[0]
        ret += f'{gnp}\n'
        ret += self._get_suggest(gnp)
        ret += '-----------------------\n\n'
        return ret

    def get_ten_years(self):
        retry_times = 10
        while retry_times > 0:
            try:
                r = requests.get(URL1, headers=HEADERS1)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'åå¹´æœŸå›½å€ºï¼š\n'

        data = r.content.decode('gbk').split(',')
        ten_rate=''
        if len(data) > 3:
            ten_rate = data[3]
        ret += f'{ten_rate}\n'
        if len(ten_rate) > 0 and float(ten_rate) < 3.0 and float(ten_rate) > 2.7:
            ret += 'æ­¤æ—¶å€ºåˆ¸åŠå…¶æ²¡æœ‰æ€§ä»·æ¯”ï¼Œä¸å»ºè®®ä¹°å…¥\n'
        elif len(ten_rate) > 0 and float(ten_rate) < 2.7:
            ret += 'å»ºè®®åˆ†æ‰¹å–å‡ºå€ºåˆ¸\n'
        ret += '-----------------------\n\n'
        return ret

    def _get_suggest(self, gnp: str) -> str:
        start = gnp.find('ï¼š')
        end = gnp.find('%')

        if start == -1 or end == -1:
            return ''

        gnp = float(gnp[start+1:end])
        if gnp > 80.0:
            return 'æ­¤æ—¶ä¸å»ºè®®ä¹°å…¥ä»»ä½•è‚¡ç¥¨ï¼Œä¹°äº†å°±æ˜¯å¤§å‚»ç“œ\n'

        if gnp < 65.0:
            return 'æ­¤æ—¶æœºä¼šåƒè½½éš¾é€¢\n'

        if gnp < 70.0:
            return 'æ­¤æ—¶ä¸ä¹°æ›´å¾…ä½•æ—¶ï¼Œç»ä½³ä¹°ç‚¹ï¼Œä¸ä¹°å°±æ˜¯å‚»å­\n'

        if gnp < 75.0:
            return 'æ­¤æ—¶é€‚åˆä¹°å…¥\n'

        if gnp < 80.0:
            return 'æ­¤æ—¶é€‚åˆå®šæŠ•ï¼Œå°‘é‡ä¹°å…¥\n'

    def get_sh(self):
        retry_times = 10
        while retry_times > 0:
            try:
                r = requests.get(URL2, headers=HEADERS)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'æ’ç”ŸæŒ‡æ•°ï¼š'
        json_data = r.json()
        data = json_data['data'][0]
        ret += str(data['current'])
        ret += '\nä¸Šè¯æŒ‡æ•°ï¼š'
        data = json_data['data'][1]
        ret += str(data['current'])
        ret += '\nçº³æ–¯è¾¾å…‹æŒ‡æ•°ï¼š'
        data = json_data['data'][2]
        ret += str(data['current'])
        ret += '\n-----------------------\n\n'
        return ret


# c = Crawler()
# print(c.monitor())
