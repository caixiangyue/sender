import requests
import time
import asyncio
from lxml import etree

from common.utils import HEADERS
# HEADERS = {"User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/109.0.0.0 Safari/537.36"}
HEADERS1 = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/72.0.3626.121 Safari/537.36", "Referer":"https://stock.finance.sina.com.cn/forex/globalbd/gcny10.html"}

URL = 'https://legulegu.com/stockdata/marketcap-gdp'
URL1 = 'https://hq.sinajs.cn/?rn=1670838672882&list=globalbd_gcny10'
URL2 = 'https://stock.xueqiu.com/v5/stock/realtime/quotec.json?symbol=HKHSI,SH000001,.IXIC'
URL3 = 'https://xueqiu.com'
URL4 = 'https://stock.xueqiu.com/v5/stock/quote.json?symbol='
ZSH = 'SH600028'
NH = 'SH601288'
DQTL = 'SH601006'
YLGF = 'SH600887'
HLSN = 'SH600585'
SHFZ = 'SZ000895'
ZSYH = 'SH600036'
GZMT = 'SH600519'
YGER = 'SH600177'
TSG = 'SH601000'
TXKG = '00700'
LRZY = 'SH600285'
CJDL = 'SH600900'
SHJC = 'SH600009'
GLYY = 'SH603087'
SXG = 'SH603896'
YZGF = 'SH603886'
async def get_data_current(name, cookies, code):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(f'{URL4}{code}', headers=HEADERS, cookies=cookies)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        json_data = r.json()
        if json_data is None:
            return ''
        current = json_data['data']['quote']['current']
        chg = json_data['data']['quote']['chg']
        if chg >= 0.0:
            chg_str = f'ğŸ“ˆ{chg}'
        else:
            chg_str = f'ğŸ“‰{abs(chg)}'
        wave = round((abs(chg) / current) * 100, 2)
        wave_str = ''
        if wave > 1.0:
            wave_str = f'ï¼Œæ³¢åŠ¨{str(wave)}ä¸ªç‚¹'

        ret = f"{name}: {str(current)}ï¼Œ{chg_str}{wave_str}"
        print(ret)
        return ret

async def get_all(cookie):
    f = await asyncio.gather(
        get_data_current('ä¼Šåˆ©è‚¡ä»½', cookie, YLGF),
        get_data_current('æµ·èºæ°´æ³¥', cookie, HLSN),
        get_data_current('åŒæ±‡å‘å±•', cookie, SHFZ),
        get_data_current('å†œä¸šé“¶è¡Œ', cookie, NH),
        get_data_current('ä¸­å›½çŸ³åŒ–', cookie, ZSH),
        get_data_current('å¤§ç§¦é“è·¯', cookie, DQTL),
        get_data_current('æ‹›å•†é“¶è¡Œ', cookie, ZSYH),
        get_data_current('è´µå·èŒ…å°', cookie, GZMT),
        get_data_current('è…¾è®¯æ§è‚¡', cookie, TXKG),
        get_data_current('ç¾šé”åˆ¶è¯', cookie, LRZY),
        get_data_current('ç”˜æè¯ä¸š', cookie, GLYY),
        get_data_current('é•¿æ±Ÿç”µåŠ›', cookie, CJDL),
        get_data_current('ä¸Šæµ·æœºåœº', cookie, SHJC),
        get_data_current('å…ƒç¥–è‚¡ä»½', cookie, YZGF),
        get_data_current('é›…æˆˆå°”', cookie, YGER),
        get_data_current('å”å±±æ¸¯', cookie, TSG),
        get_data_current('å¯¿ä»™è°·', cookie, SXG),
    )
    return f
class Crawler:
    def __init__(self) -> None:
        pass

    def monitor(self):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(URL3, headers=HEADERS)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        # ret = 'ä¸­å›½çŸ³åŒ–ï¼š'
        # ret += self._get_data_current(r.cookies, ZSH)
        # ret += '\n'
        # ret += 'å†œä¸šé“¶è¡Œï¼š'
        # ret += self._get_data_current(r.cookies, NH)
        # ret += '\n'
        # ret += 'å¤§ç§¦é“è·¯ï¼š'
        # ret += self._get_data_current(r.cookies, DQTL)
        # ret += '\n---------------\n'
        f = asyncio.run(get_all(r.cookies))
        return '\n'.join(f)

    def _get_data_current(self,cookies, code):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(f'{URL4}{code}', headers=HEADERS, cookies=cookies)
                if r.status_code != 200:
                    print(r.status_code)
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)
        json_data = r.json()
        if json_data is None:
            return ''
        return str(json_data['data']['quote']['current'])


    def get_gnp(self):
        retry_times = 3
        while retry_times > 0:
            try:
                r = requests.get(URL, headers=HEADERS)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'è·å–å·´è²ç‰¹æŒ‡æ•°ï¼š\n'
        dom = etree.HTML(r.content)
        data = dom.xpath('//*[@id="data-description"]/text()')
        if len(data) > 0:
            gnp = data[0]
        ret += f'{gnp}\n'
        ret += self._get_suggest(gnp)
        ret += '-----------------------\n\n'
        return ret

    def get_ten_years(self):
        retry_times = 10
        while retry_times > 0:
            try:
                r = requests.get(URL1, headers=HEADERS1)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'åå¹´æœŸå›½å€ºï¼š\n'

        data = r.content.decode('gbk').split(',')
        ten_rate=''
        if len(data) > 3:
            ten_rate = data[3]
        ret += f'{ten_rate}\n'
        if len(ten_rate) > 0 and float(ten_rate) < 3.0 and float(ten_rate) > 2.7:
            ret += 'æ­¤æ—¶å€ºåˆ¸åŠå…¶æ²¡æœ‰æ€§ä»·æ¯”ï¼Œä¸å»ºè®®ä¹°å…¥\n'
        elif len(ten_rate) > 0 and float(ten_rate) < 2.7:
            ret += 'å»ºè®®åˆ†æ‰¹å–å‡ºå€ºåˆ¸\n'
        ret += '-----------------------\n\n'
        return ret

    def _get_suggest(self, gnp: str) -> str:
        start = gnp.find('ï¼š')
        end = gnp.find('%')

        if start == -1 or end == -1:
            return ''

        gnp = float(gnp[start+1:end])
        if gnp > 80.0:
            return 'æ­¤æ—¶ä¸å»ºè®®ä¹°å…¥ä»»ä½•è‚¡ç¥¨ï¼Œä¹°äº†å°±æ˜¯å¤§å‚»ç“œ\n'

        if gnp < 65.0:
            return 'æ­¤æ—¶æœºä¼šåƒè½½éš¾é€¢\n'

        if gnp < 70.0:
            return 'æ­¤æ—¶ä¸ä¹°æ›´å¾…ä½•æ—¶ï¼Œç»ä½³ä¹°ç‚¹ï¼Œä¸ä¹°å°±æ˜¯å‚»å­\n'

        if gnp < 75.0:
            return 'æ­¤æ—¶é€‚åˆä¹°å…¥\n'

        if gnp < 80.0:
            return 'æ­¤æ—¶é€‚åˆå®šæŠ•ï¼Œå°‘é‡ä¹°å…¥\n'

    def get_sh(self):
        retry_times = 10
        while retry_times > 0:
            try:
                r = requests.get(URL2, headers=HEADERS)
                if r.status_code != 200:
                    retry_times -= 1
                    continue
                break
            except Exception as e:
                retry_times -= 1
                print(e)
                if retry_times == 0:
                    return ''
                time.sleep(1)

        ret = 'æ’ç”ŸæŒ‡æ•°ï¼š'
        json_data = r.json()
        data = json_data['data'][0]
        ret += str(data['current'])
        ret += '\nä¸Šè¯æŒ‡æ•°ï¼š'
        data = json_data['data'][1]
        ret += str(data['current'])
        ret += '\nçº³æ–¯è¾¾å…‹æŒ‡æ•°ï¼š'
        data = json_data['data'][2]
        ret += str(data['current'])
        ret += '\n-----------------------\n\n'
        return ret


# c = Crawler()
# print(c.monitor())
